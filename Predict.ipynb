{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "classes = ('anger','disgust','fear', 'happy', 'sad', 'surprised', 'normal')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(\n",
    "    \"D:\\surfa\\Documents\\Project\\EmotionInsight\\Trained_Model\\CONVNEXT62_0.696.pth\"\n",
    ")\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# path = 'datasets/test/0/'\n",
    "# testList = os.listdir(path)\n",
    "# for file in testList:\n",
    "#     img = Image.open(path + file)\n",
    "#     img = img.convert('RGB')\n",
    "#     img = transform_test(img)\n",
    "#     img.unsqueeze_(0)\n",
    "#     img = Variable(img).to(DEVICE)\n",
    "#     out = model(img)\n",
    "#     # Predict\n",
    "#     _, pred = torch.max(out.data, 1)\n",
    "#     print('Image Name:{},predict:{}'.format(file, classes[pred.data.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_R = Image.open(r\"D:\\surfa\\Desktop\\20220519_232859.jpg\")\n",
    "img_R = Image.open(r\"D:\\surfa\\Pictures\\5781FB179A177D469C25CA543C1C282A.jpg\")\n",
    "# img_R = Image.open(r\"D:\\surfa\\Pictures\\1493bd7f69a2ac55.jpg\")\n",
    "img = img_R.convert('RGB')\n",
    "img = transform_test(img)\n",
    "img.unsqueeze_(0)\n",
    "img = Variable(img).to(DEVICE)\n",
    "out = model(img)\n",
    "# Predict\n",
    "_, pred = torch.max(out.data, 1)\n",
    "print('predict:{}'.format(classes[pred.data.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "sub = fig.add_subplot(1,1,1)\n",
    "res = classes[pred.data.item()]\n",
    "sub.set_title(str(res))\n",
    "plt.axis('off')\n",
    "plt.imshow(img_R)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a798a8c48037f1867761d0f6ae6ee6f6529b8b462e421325d143d37b87845564"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
