{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchtoolbox.transform import Cutout\n",
    "from torchtoolbox.tools import mixup_data, mixup_criterion\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "batchSize = 100\n",
    "workers = 16\n",
    "epochs = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfrom = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandAugment(),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    Cutout(0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "transfromTest = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandAugment(),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    Cutout(0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6}\n",
    "classes = len(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeedlingData(data.Dataset):\n",
    "    def __init__(self, root, transforms=None, train=True, test=False):\n",
    "        self.test = test\n",
    "        self.transforms = transforms\n",
    "        if self.test:\n",
    "            imgs = [os.path.join(root, img) for img in os.listdir(root)]\n",
    "            self.imgs = imgs\n",
    "        else:\n",
    "            img_labels = [os.path.join(root, img) for img in os.listdir(root)]\n",
    "            imgs = []\n",
    "            for imglabel in img_labels:\n",
    "                for imgname in os.listdir(imglabel):\n",
    "                    imgpath = os.path.join(imglabel, imgname)\n",
    "                    imgs.append(imgpath)\n",
    "            trainval_file, val_file = train_test_split(imgs,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=42)\n",
    "            if train:\n",
    "                self.imgs = trainval_file\n",
    "            else:\n",
    "                self.imgs = val_file\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        img_path = img_path.replace(\"\\\\\", \"/\")\n",
    "        if self.test:\n",
    "            label = -1\n",
    "        else:\n",
    "            labelname = img_path.split('/')[-2]\n",
    "            label = Labels[labelname]\n",
    "        data = Image.open(img_path).convert('RGB')\n",
    "        data = self.transforms(data)\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SeedlingData('./datasets/train',\n",
    "                             transforms=transfrom,\n",
    "                             train=True)\n",
    "dataset_test = SeedlingData('./datasets/train',\n",
    "                            transforms=transfrom,\n",
    "                            train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batchSize,\n",
    "    #num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset_test,\n",
    "    batch_size=batchSize,\n",
    "    #num_workers=workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias,\n",
    "                         self.eps)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.convnext import convnext_base\n",
    "\n",
    "CONVNEXT = convnext_base(pretrained=False)\n",
    "print(CONVNEXT)\n",
    "CONVNEXT.norm = nn.LayerNorm(1024, eps=1e-06, elementwise_affine=True)\n",
    "#torch.save(CONVNEXT,'1.pth')\n",
    "CONVNEXT.head = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1024, 512,True), nn.GELU(), nn.Dropout(0.2), nn.Linear(512, 7,True),\n",
    "    nn.LogSoftmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVNEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALEX = models.alexnet(pretrained=True, progress=True)\n",
    "# ALEX.classifier[6] == nn.Linear(4096, classes)\n",
    "# VGG = models.vgg19(pretrained=True, progress=True)\n",
    "# VGG.classifier[6] == nn.Linear(4096, classes)\n",
    "\n",
    "# DENSENET = models.densenet169(pretrained=True, progress=True)\n",
    "# DENSENET.classifier = nn.Linear(1664, 7, True)\n",
    "\n",
    "# RESNET = models.resnet50(pretrained=True, progress=True)\n",
    "# channel_in = RESNET.fc.in_features\n",
    "# RESNET.fc = nn.Linear(channel_in, classes)\n",
    "# MOBILE = models.mobilenet_v3_large(pretrained=True, progress=True)\n",
    "# print(MOBILE)\n",
    "# INCEPTION = models.inception_v3(pretrained=True,progress=True,num_classes=classes,init_weights=True)\n",
    "\n",
    "# GOOGLE = models.googlenet(pretrained=True,progress=True,num_classes=classes,init_weights=True)\n",
    "# MNASNET = models.mnasnet1_0(pretrained=True,progress=True,num_classes=classes)\n",
    "\n",
    "# modelPool = [ALEX, VGG, DENSENET, RESNET, MOBILE, CONVNEXT]\n",
    "# namePool = ['ALEX', 'VGG', 'DENSENET', 'RESNET', 'MOBILE', 'CONVNEXT']\n",
    "modelPool = [CONVNEXT]\n",
    "namePool = ['CONVNEXT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    lr_now = lr\n",
    "    total_num = len(train_loader.dataset)\n",
    "    print(total_num, len(train_loader))\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device,\n",
    "                               non_blocking=True), target.to(device,\n",
    "                                                             non_blocking=True)\n",
    "        data, labels_a, labels_b, lam = mixup_data(data, target, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        # output = model(data)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = mixup_criterion(criterion, model(data), labels_a, labels_b,\n",
    "                                   lam)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        #lr_now = scheduler.get_last_lr()\n",
    "        scaler.update()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        print_loss = loss.data.item()\n",
    "        sum_loss += print_loss\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tlr={}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader), loss.item(),lr_now))\n",
    "    ave_loss = sum_loss / len(train_loader)\n",
    "    writer.add_scalar('loss',loss,epoch)\n",
    "    writer.add_scalar('lr',lr_now,epoch)\n",
    "    print('Epoch:{},loss:{},lr:{}'.format(epoch, ave_loss, lr_now))\n",
    "\n",
    "\n",
    "# 验证过程\n",
    "def val(model, device, test_loader):\n",
    "    global ACC\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total_num = len(test_loader.dataset)\n",
    "    print(total_num, len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data).to(device), Variable(target).to(\n",
    "                device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            correct += torch.sum(pred == target)\n",
    "            print_loss = loss.data.item()\n",
    "            test_loss += print_loss\n",
    "        correct = correct.data.item()\n",
    "        acc = correct / total_num\n",
    "        avgloss = test_loss / len(test_loader)\n",
    "        LOSS_LIST.append(avgloss)\n",
    "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "              format(avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
    "        ACC_LIST.append(acc)\n",
    "        if acc > ACC:\n",
    "            torch.save(\n",
    "                model, namePool[EPOCHS_COUNT] + '/' + namePool[EPOCHS_COUNT] +\n",
    "                str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
    "            ACC = acc\n",
    "        writer.add_scalar('Accuracy',acc,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = 0\n",
    "for model in modelPool:\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = SoftTargetCrossEntropy()\n",
    "    # model.fc = nn.Sequential(nn.Linear(2048,1024), nn.ReLU(), nn.Dropout(0.2),\n",
    "    #                          nn.Linear(512, 7), nn.LogSoftmax(dim=1))\n",
    "    # model.fc = nn.Sequential(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    model.to(device)\n",
    "    # 选择简单暴力的Adam优化器，学习率调低\n",
    "    #optimizer = optim.Adam(model_ft.parameters(), lr=modellr)\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=lr)\n",
    "    #optimizer = optim.RAdam(model.parameters(), lr=lr)\n",
    "    cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
    "                                                           T_max=20,\n",
    "                                                           eta_min=1e-9)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.01,\n",
    "        steps_per_epoch=len(dataset_train),\n",
    "        epochs=epochs\n",
    "        )\n",
    "    alpha = 0.2\n",
    "\n",
    "    EPOCHS_COUNT = 0\n",
    "    ACC_LIST = []\n",
    "    LOSS_LIST = []\n",
    "    ACC = 0\n",
    "\n",
    "    if os.path.isdir(namePool[MODEL]):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(namePool[MODEL])\n",
    "        MODEL += 1\n",
    "    # 训练\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        cosine_schedule.step()\n",
    "        val(model, device, test_loader)\n",
    "\n",
    "    sns.set(palette='twilight')\n",
    "    sns.relplot(kind='line', data=ACC_LIST)\n",
    "    plt.xlabel(\"Epoch Time\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    #plt.ylim(top=1,bottom=0)\n",
    "    sns.relplot(kind='line', data=LOSS_LIST)\n",
    "    plt.xlabel(\"Epoch Time\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    print(ACC_LIST)\n",
    "    print(LOSS_LIST)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a798a8c48037f1867761d0f6ae6ee6f6529b8b462e421325d143d37b87845564"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
